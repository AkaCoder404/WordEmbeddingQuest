{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Here, we evaluate our trained embeddings on downstream metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from six import iteritems\n",
    "\n",
    "from utils import standardize_string, embedding_info, load_word_vectors, most_similar_words\n",
    "from embedding import Embedding\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "# from sklearn.datasets.base import Bunch\n",
    "from sklearn.datasets._base import Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_analogy_vec(embeddings: Word2Vec, word1, word2, word3):\n",
    "    \"\"\"\"\"\"\n",
    "    try:\n",
    "        w1_vec = embeddings.wv[word1]\n",
    "        w2_vec = embeddings.wv[word2]\n",
    "        w3_vec = embeddings.wv[word3]\n",
    "        \n",
    "        analogy_vec = w1_vec - w2_vec + w3_vec\n",
    "        return analogy_vec\n",
    "    except Exception as e:\n",
    "        return False\n",
    "        \n",
    "# Download bencharmks\n",
    "def download_file(url, save_path):\n",
    "    \"\"\"\"\"\"\n",
    "        \n",
    "# Anology Benchmarks\n",
    "def get_google_analogy_benchmark():\n",
    "    \"\"\"\n",
    "    Testing both semantic and syntactic analogies.\n",
    "    \"\"\"\n",
    "    dataset = \"EN-GOOGLE.txt\"\n",
    "    # If not at location data/EN-GOOGLE.txt, download it\n",
    "    \n",
    "    with open(\"data/EN-GOOGLE.txt\", \"r\") as f:\n",
    "        L = f.read().splitlines()\n",
    "\n",
    "    # Simple 4 word analogy questions with categories\n",
    "    questions = []\n",
    "    answers = []        \n",
    "    category = []       # categories of analogies\n",
    "    cat = None\n",
    "    for l in L:\n",
    "        if l.startswith(\":\"):\n",
    "            cat =l.lower().split()[1]\n",
    "        else:\n",
    "            words =  standardize_string(l).split()\n",
    "            questions.append(words[0:3])\n",
    "            answers.append(words[3])\n",
    "            category.append(cat)\n",
    "            \n",
    "            \n",
    "    print(f\"There are {len(questions)} questions\")\n",
    "    print(f\"There are {len(set(category))} categories\")\n",
    "    category_distribution = Counter(category)\n",
    "    print(\"Disribution\", category_distribution)\n",
    "    return questions, answers, category\n",
    "\n",
    "def get_msr_benchmark():\n",
    "    \"\"\"\n",
    "    Test performance on syntatic analygies\n",
    "    \"\"\"\n",
    "    with open(\"data/EN-MSR.txt\", \"r\") as f:\n",
    "        L = f.read().splitlines()\n",
    "        \n",
    "    questions = []\n",
    "    answers = []\n",
    "    category = []\n",
    "    for l in L:\n",
    "        words = standardize_string(l).split()\n",
    "        questions.append(words[0:3])\n",
    "        answers.append(words[4])\n",
    "        category.append(words[3])\n",
    "\n",
    "    verb = set([c for c in set(category) if c.startswith(\"VB\")])\n",
    "    noun = set([c for c in set(category) if c.startswith(\"NN\")])\n",
    "    category_high_level = []\n",
    "    for cat in category:\n",
    "         if cat in verb:\n",
    "             category_high_level.append(\"verb\")\n",
    "         elif cat in noun:\n",
    "             category_high_level.append(\"noun\")\n",
    "         else:\n",
    "             category_high_level.append(\"adjective\")\n",
    "             \n",
    "    print(f\"There are {len(questions)} questions\")\n",
    "    print(f\"There are {len(set(category))} categories\")\n",
    "    category_distribution = Counter(category)\n",
    "    print(\"Disribution\", category_distribution)\n",
    "    return questions, answers, category\n",
    "\n",
    "def get_wordrep_benchmark():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "def get_semeval2012_benchmark():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "# Similarity Tasks\n",
    "def get_ws353_benchmark():\n",
    "    \"\"\"\n",
    "    Test performance on similarity\n",
    "    \"\"\"\n",
    "    path = \"./data/WORDSIM353/combined.csv\" # or combined.tab\n",
    "    \n",
    "    dataset = pd.read_csv(path)\n",
    "    \n",
    "    words = []\n",
    "    human_scores = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        word1, word2 = row['Word 1'], row['Word 2']\n",
    "        human_similarity = row['Human (mean)']\n",
    "\n",
    "        words.append([word1, word2])\n",
    "        human_scores.append(human_similarity)\n",
    "        \n",
    "    return Bunch(X=words, y=human_scores)\n",
    "    # return words, human_scores\n",
    "\n",
    "def get_simlex999_benchmark():\n",
    "    \"\"\"\n",
    "    SimLex999 dataset for testing attributional similarity\n",
    "    \"\"\"\n",
    "    data_path = \"./data\"\n",
    "    dataset = \"EN-SIM999\"\n",
    "    data = pd.read_csv(f'{data_path}/{dataset}.txt', sep=\"\\t\")\n",
    "    # We basically select all the columns available\n",
    "    X = data[['word1', 'word2']].values\n",
    "    y = data['SimLex999'].values\n",
    "    sd = data['SD(SimLex)'].values\n",
    "    conc = data[['conc(w1)', 'conc(w2)', 'concQ']].values\n",
    "    POS = data[['POS']].values\n",
    "    assoc = data[['Assoc(USF)', 'SimAssoc333']].values\n",
    "    return Bunch(X=X.astype(\"object\"), y=y, sd=sd, conc=conc, POS=POS, assoc=assoc)\n",
    "    \n",
    "\n",
    "def get_RG65_benchmark():\n",
    "    \"\"\"\n",
    "    Rubenstein and Goodenough dataset for testing attributional and\n",
    "    relatedness similarity\n",
    "    \"\"\"\n",
    "    data_path = \"./data\"\n",
    "    dataset = \"EN-RG-65\"\n",
    "    data = pd.read_csv(f'{data_path}/{dataset}.txt', header=None, sep=\"\\t\").values\n",
    "    return Bunch(X=data[:, 0:2].astype(\"object\"),\n",
    "                 y=data[:, 2].astype(float),\n",
    "                 sd=np.std(data[:, 3:].astype(float)))\n",
    "   \n",
    "    \n",
    "def get_RW_benchmark():\n",
    "    data_path = \"./data\"\n",
    "    dataset = \"EN-RW\"\n",
    "    data = pd.read_csv(f'{data_path}/{dataset}.txt', header=None, sep=\"\\t\").values\n",
    "    return Bunch(X=data[:, 0:2].astype(\"object\"),\n",
    "                 y=data[:, 2].astype(float),\n",
    "                 sd=np.std(data[:, 3:].astype(float)))\n",
    "    \n",
    "def get_MTurk_benchmark():\n",
    "    \"\"\"\n",
    "    MTurk dataset for testing attributional similarity\n",
    "    \"\"\"\n",
    "    data_path = \"./data\"\n",
    "    dataset = \"EN-TRUK\"\n",
    "    data = pd.read_csv(f'{data_path}/{dataset}.txt', header=None, sep=\" \").values\n",
    "    return Bunch(X=data[:, 0:2].astype(\"object\"),\n",
    "                 y=2 * data[:, 2].astype(float))\n",
    "    \n",
    "\n",
    "# Categorization Tasks\n",
    "def calculate_purity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate purity for given true and predicted cluster labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array, shape: (n_samples, 1)\n",
    "      True cluster labels\n",
    "\n",
    "    y_pred: array, shape: (n_samples, 1)\n",
    "      Cluster assingment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    purity: float\n",
    "      Calculated purity.\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    true_clusters = np.zeros(shape=(len(set(y_true)), len(y_true)))\n",
    "    pred_clusters = np.zeros_like(true_clusters)\n",
    "    for id, cl in enumerate(set(y_true)):\n",
    "        true_clusters[id] = (y_true == cl).astype(\"int\")\n",
    "    for id, cl in enumerate(set(y_pred)):\n",
    "        pred_clusters[id] = (y_pred == cl).astype(\"int\")\n",
    "\n",
    "    M = pred_clusters.dot(true_clusters.T)\n",
    "    return 1. / len(y_true) * np.sum(np.max(M, axis=1))\n",
    "\n",
    "def evaluate_categorization(w, X, y, method=\"all\", seed=None):\n",
    "    \"\"\"\n",
    "    Evaluate embeddings on categorization task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w: Embedding or dict\n",
    "      Embedding to test.\n",
    "\n",
    "    X: vector, shape: (n_samples, )\n",
    "      Vector of words.\n",
    "\n",
    "    y: vector, shape: (n_samples, )\n",
    "      Vector of cluster assignments.\n",
    "\n",
    "    method: string, default: \"all\"\n",
    "      What method to use. Possible values are \"agglomerative\", \"kmeans\", \"all.\n",
    "      If \"agglomerative\" is passed, method will fit AgglomerativeClustering (with very crude\n",
    "      hyperparameter tuning to avoid overfitting).\n",
    "      If \"kmeans\" is passed, method will fit KMeans.\n",
    "      In both cases number of clusters is preset to the correct value.\n",
    "\n",
    "    seed: int, default: None\n",
    "      Seed passed to KMeans.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    purity: float\n",
    "      Purity of the best obtained clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(w, dict):\n",
    "        print(\"Convert to Embedding\")\n",
    "        w = Embedding.from_dict(w)\n",
    "\n",
    "    assert method in [\"all\", \"kmeans\", \"agglomerative\"], \"Uncrecognized method\"\n",
    "\n",
    "    mean_vector = np.mean(w.vectors, axis=0, keepdims=True)\n",
    "    words = np.vstack([w.get(word, mean_vector) for word in X.flatten()])\n",
    "    ids = np.random.RandomState(seed).choice(range(len(X)), len(X), replace=False)\n",
    "\n",
    "    # Evaluate clustering on several hyperparameters of AgglomerativeClustering and\n",
    "    # KMeans\n",
    "    best_purity = 0\n",
    "\n",
    "    if method == \"all\" or method == \"agglomerative\":\n",
    "        best_purity = calculate_purity(y[ids], AgglomerativeClustering(n_clusters=len(set(y)),\n",
    "                                                                       metric=\"euclidean\",\n",
    "                                                                       linkage=\"ward\").fit_predict(words[ids]))\n",
    "        # logger.debug(\"Purity={:.3f} using affinity={} linkage={}\".format(best_purity, 'euclidean', 'ward'))\n",
    "        for metric in [\"cosine\", \"euclidean\"]:\n",
    "            for linkage in [\"average\", \"complete\"]:\n",
    "                purity = calculate_purity(y[ids], AgglomerativeClustering(n_clusters=len(set(y)),\n",
    "                                                                          metric=metric,\n",
    "                                                                          linkage=linkage).fit_predict(words[ids]))\n",
    "                # logger.debug(\"Purity={:.3f} using affinity={} linkage={}\".format(purity, affinity, linkage))\n",
    "                best_purity = max(best_purity, purity)\n",
    "\n",
    "    if method == \"all\" or method == \"kmeans\":\n",
    "        purity = calculate_purity(y[ids], KMeans(random_state=seed, n_init=10, n_clusters=len(set(y))).\n",
    "                                  fit_predict(words[ids]))\n",
    "        # logger.debug(\"Purity={:.3f} using KMeans\".format(purity))\n",
    "        best_purity = max(purity, best_purity)\n",
    "\n",
    "    return best_purity\n",
    "\n",
    "def get_cluster_assignments(dataset_name, sep=\" \", skip_header=False):\n",
    "    data_dir = \"./data\"\n",
    "    files = glob.glob(os.path.join(data_dir, dataset_name + \"/*.txt\"))\n",
    "    X = []\n",
    "    y = []\n",
    "    names = []\n",
    "    for cluster_id, file_name in enumerate(files):\n",
    "        with open(file_name) as f:\n",
    "            lines = f.read().splitlines()[(int(skip_header)):]\n",
    "\n",
    "            X += [l.split(sep) for l in lines]\n",
    "            y += [os.path.basename(file_name).split(\".\")[0]] * len(lines)\n",
    "    return Bunch(X=np.array(X, dtype=\"object\"), y=np.array(y).astype(\"object\"))\n",
    "    \n",
    "def get_ap_benchmark():\n",
    "    \"\"\"\n",
    "    Test performance on Almuhareb and Abdulrahman categorization dataset\n",
    "    \"\"\"\n",
    "    return get_cluster_assignments(\"EN-AP\")\n",
    "\n",
    "def get_bless_benchmark():\n",
    "    \"\"\"\n",
    "    Baroni and Marco categorization dataset\n",
    "    \"\"\"\n",
    "    return get_cluster_assignments(\"EN-BLESS\")\n",
    "  \n",
    "def get_battig_benchmark():\n",
    "    \"\"\"\n",
    "    Fetch 1969 Battig dataset\n",
    "    \"\"\"\n",
    "    data = get_cluster_assignments(\"EN-BATTIG\", sep=\",\", skip_header=True)\n",
    "    return Bunch(X=data.X[:, 0], y=data.y,freq=data.X[:, 1], frequency=data.X[:, 2], rank=data.X[:, 3], rfreq=data.X[:, 4])\n",
    "\n",
    "def get_essli_2c_benchmark():\n",
    "    \"\"\"\n",
    "    ESSLI 2c task categorization dataset\n",
    "    \"\"\"\n",
    "    return get_cluster_assignments(\"EN-ESSLI-2c\")\n",
    "    \n",
    "def get_essli_2b_benchmark():\n",
    "    \"\"\"\n",
    "    ESSLI 2b task categorization dataset\n",
    "    \"\"\"\n",
    "    return get_cluster_assignments(\"EN-ESSLI-2b\")\n",
    "    \n",
    "def get_essli_1a_benchmark():\n",
    "    \"\"\"\n",
    "    ESSLI 1a task categorization dataset\n",
    "    \"\"\"\n",
    "    return get_cluster_assignments(\"EN-ESSLI-1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Dialog\n",
    "\n",
    "Let's first do some analogy benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailydialog_word2vec = Word2Vec.load('./out/dailydialog_word2vec_100.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19544 questions\n",
      "There are 14 categories\n",
      "Disribution Counter({'capital-world': 4524, 'city-in-state': 2467, 'gram6-nationality-adjective': 1599, 'gram7-past-tense': 1560, 'gram3-comparative': 1332, 'gram8-plural': 1332, 'gram4-superlative': 1122, 'gram5-present-participle': 1056, 'gram1-adjective-to-adverb': 992, 'gram9-plural-verbs': 870, 'currency': 866, 'gram2-opposite': 812, 'capital-common-countries': 506, 'family': 506})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19544/19544 [00:15<00:00, 1298.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 170\n",
      "Analogies attempted: 8618\n",
      "Correct_per_category {'capital-common-countries': 3, 'capital-world': 3, 'family': 155, 'gram8-plural': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions, answers, category = get_google_analogy_benchmark() # google analogy benchmark\n",
    "# questions, answers, category = get_msr_benchmark() # msr benchmark\n",
    "\n",
    "subset = range(0, len(questions))\n",
    "correct_total_count, analogies_attempted = 0, 0\n",
    "correct_per_category = {}     # Calculate correct per category\n",
    "\n",
    "for index in tqdm(subset):\n",
    "    word1, word2, word3 = questions[index]\n",
    "    answer = answers[index]\n",
    "\n",
    "    analogy_vec = get_analogy_vec(dailydialog_word2vec, word1, word2, word3)\n",
    "    if analogy_vec is False:\n",
    "        continue\n",
    "    \n",
    "    # Get similar vectors\n",
    "    similar_vectors = dailydialog_word2vec.wv.most_similar(positive=[analogy_vec])\n",
    "    # Considered correct if answer in top K = 5 of most similar\n",
    "    correct = True if answer in [word for word, _ in similar_vectors[:5]] else False\n",
    "    \n",
    "    analogies_attempted += 1\n",
    "    if correct:\n",
    "        correct_total_count += 1\n",
    "        # print(\"-\" * 50)\n",
    "        # print(f\"Category is f{category[index]}\")\n",
    "        # print(f\"Q: {word1} is to {word2} as {word3} is to ?\")\n",
    "        # print(f\"A: {answer}\")\n",
    "        # print(f\"P: {similar_vectors}\")\n",
    "        if category[index] not in correct_per_category:\n",
    "            correct_per_category[category[index]] = 0\n",
    "        else:\n",
    "            correct_per_category[category[index]] += 1\n",
    "\n",
    "print(\"Total correct:\", correct_total_count)\n",
    "print(\"Analogies attempted:\", analogies_attempted)\n",
    "print(\"Correct_per_category\", correct_per_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's perform a word similarity benchmarks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS353\n",
      "--------------------------------------------------\n",
      "total checked 280 out of 353\n",
      "Corr: 0.1874229884478066\n",
      "P-Val: 0.0016320460055392872\n",
      "SimLex999\n",
      "--------------------------------------------------\n",
      "total checked 899 out of 999\n",
      "Corr: 0.048256811542747344\n",
      "P-Val: 0.14825320993622756\n",
      "RG65\n",
      "--------------------------------------------------\n",
      "total checked 35 out of 65\n",
      "Corr: 0.04020170931722092\n",
      "P-Val: 0.8186423720913868\n",
      "RW\n",
      "--------------------------------------------------\n",
      "total checked 279 out of 2034\n",
      "Corr: 0.03797309230703268\n",
      "P-Val: 0.5276117216290732\n",
      "MTurk\n",
      "--------------------------------------------------\n",
      "total checked 186 out of 287\n",
      "Corr: 0.057520211590356474\n",
      "P-Val: 0.4354917272912583\n",
      "      WS353  SimLex999      RG65        RW    MTurk\n",
      "0  0.187423   0.048257  0.040202  0.037973  0.05752\n"
     ]
    }
   ],
   "source": [
    "similarity_tasks = {\n",
    "   \"WS353\" : get_ws353_benchmark(),\n",
    "   \"SimLex999\": get_simlex999_benchmark(),\n",
    "   \"RG65\": get_RG65_benchmark(),\n",
    "   \"RW\": get_RW_benchmark(),\n",
    "   \"MTurk\": get_MTurk_benchmark()\n",
    "}\n",
    "\n",
    "similarity_results = {}\n",
    "for task, data in iteritems(similarity_tasks):\n",
    "    print(task)\n",
    "    sim_words, sim_scores = data.X, data.y\n",
    "\n",
    "    human_scores = []\n",
    "    model_scores = []\n",
    "    total_checked = 0\n",
    "\n",
    "    for i, words in enumerate(sim_words):\n",
    "        word1, word2 = words[0], words[1]\n",
    "    \n",
    "        # Check if words exist in embedding dictionary\n",
    "        try:\n",
    "            word1_vec = dailydialog_word2vec.wv[word1]\n",
    "            word2_vec = dailydialog_word2vec.wv[word2]\n",
    "            similarity = 10 * cosine_similarity(word1_vec.reshape(1, -1), word2_vec.reshape(1, -1))\n",
    "            model_similarity = similarity[0][0]     \n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            continue\n",
    "            \n",
    "        total_checked += 1\n",
    "        # Store the scores\n",
    "        human_scores.append(sim_scores[i])\n",
    "        model_scores.append(model_similarity)\n",
    "        \n",
    "\n",
    "    model_correlation, p_value = spearmanr(human_scores, model_scores)\n",
    "    print(\"-\" * 50)\n",
    "    print('total checked', total_checked, 'out of', len(sim_scores))\n",
    "    print('Corr:',model_correlation)\n",
    "    print('P-Val:',p_value)\n",
    "    \n",
    "    similarity_results[task] = model_correlation\n",
    "    \n",
    "sim = pd.DataFrame([similarity_results])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's perform classification test benches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AP  BLESS    BATTIG  ESSLI_2c  ESSLI_2b  ESSLI_1a\n",
      "0  0.149254   0.22  0.095775  0.511111      0.45  0.431818\n"
     ]
    }
   ],
   "source": [
    "ap_bench = get_ap_benchmark()\n",
    "bless_bench = get_bless_benchmark()\n",
    "battig_bench = get_battig_benchmark()\n",
    "essli_2c_bench = get_essli_2c_benchmark()\n",
    "essli_2b_bench = get_essli_2b_benchmark()\n",
    "essli_1a_bench = get_essli_1a_benchmark()\n",
    "\n",
    "categorization_tasks = {\n",
    "    \"AP\": ap_bench,\n",
    "    \"BLESS\": bless_bench,\n",
    "    \"BATTIG\": battig_bench,\n",
    "    \"ESSLI_2c\": essli_2c_bench,\n",
    "    \"ESSLI_2b\": essli_2b_bench,\n",
    "    \"ESSLI_1a\": essli_1a_bench\n",
    "}\n",
    "\n",
    "categorization_results = {}\n",
    "w = Embedding.from_gensim_word2vec(dailydialog_word2vec)\n",
    "\n",
    "# Calculate results using helper function\n",
    "for name, data in iteritems(categorization_tasks):\n",
    "    categorization_results[name] = evaluate_categorization(w, data.X, data.y)\n",
    "    \n",
    "# Construct pd table\n",
    "cat = pd.DataFrame([categorization_results])\n",
    "print(cat)\n",
    "del categorization_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google News\n",
    "Pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=300, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "googlenews_kv: KeyedVectors = models.KeyedVectors.load_word2vec_format(\n",
    "    './data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "print(googlenews_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8000 questions\n",
      "There are 16 categories\n",
      "Disribution Counter({'jj_jjr': 500, 'jjr_jj': 500, 'jj_jjs': 500, 'jjs_jj': 500, 'jjs_jjr': 500, 'jjr_jjs': 500, 'nn_nns': 500, 'nns_nn': 500, 'nn_nnpos': 500, 'nnpos_nn': 500, 'vb_vbd': 500, 'vbd_vb': 500, 'vb_vbz': 500, 'vbz_vb': 500, 'vbz_vbd': 500, 'vbd_vbz': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 507961.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 0\n",
      "Analogies attempted: 0\n",
      "Correct_per_category {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analogy_tasks = {\n",
    "    \"GOOGLE\": \"\",\n",
    "    \"MSR\": \"\"\n",
    "}\n",
    "\n",
    "# questions, answers, category = get_google_analogy_benchmark()\n",
    "questions, answers, category = get_msr_benchmark()\n",
    "correct_total_count, analogies_attempted = 0, 0\n",
    "correct_per_category = {}     # Calculate correct per category\n",
    "for index in tqdm(range(0, len(questions))):\n",
    "    word1, word2, word3 = questions[index]\n",
    "    answer = answers[index]\n",
    "\n",
    "    try:\n",
    "        w1_vec = googlenews_kv[word1]\n",
    "        w2_vec = googlenews_kv[word2]\n",
    "        w3_vec = googlenews_kv[word3]\n",
    "        \n",
    "        analogy_vec = w1_vec - w2_vec + w3_vec\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    \n",
    "    # Get similar vectors\n",
    "    similar_vectors = googlenews_kv.most_similar(positive=[analogy_vec])\n",
    "    # Considered correct if answer in top K = 5 of most similar\n",
    "    correct = True if answer in [word for word, _ in similar_vectors[:5]] else False\n",
    "    \n",
    "    analogies_attempted += 1\n",
    "    if correct:\n",
    "        correct_total_count += 1\n",
    "        if category[index] not in correct_per_category:\n",
    "            correct_per_category[category[index]] = 0\n",
    "        else:\n",
    "            correct_per_category[category[index]] += 1\n",
    "\n",
    "print(\"Total correct:\", correct_total_count)\n",
    "print(\"Analogies attempted:\", analogies_attempted)\n",
    "print(\"Correct_per_category\", correct_per_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some similarity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "WS353\n",
      "total checked 353 out of 353\n",
      "Corr: 0.7000166486272194\n",
      "P-Val: 2.86866666051422e-53\n",
      "--------------------------------------------------\n",
      "SimLex999\n",
      "total checked 999 out of 999\n",
      "Corr: 0.44196551091403796\n",
      "P-Val: 5.068221892023142e-49\n",
      "--------------------------------------------------\n",
      "RG65\n",
      "total checked 65 out of 65\n",
      "Corr: 0.7607828603850846\n",
      "P-Val: 1.9330285740005686e-13\n",
      "--------------------------------------------------\n",
      "RW\n",
      "total checked 1825 out of 2034\n",
      "Corr: 0.5342097582319317\n",
      "P-Val: 3.409254222298205e-135\n",
      "--------------------------------------------------\n",
      "MTurk\n",
      "total checked 275 out of 287\n",
      "Corr: 0.6839689831303845\n",
      "P-Val: 2.8467829183547183e-39\n",
      "      WS353  SimLex999      RG65       RW     MTurk\n",
      "0  0.700017   0.441966  0.760783  0.53421  0.683969\n"
     ]
    }
   ],
   "source": [
    "similarity_tasks = {\n",
    "   \"WS353\" : get_ws353_benchmark(),\n",
    "   \"SimLex999\": get_simlex999_benchmark(),\n",
    "   \"RG65\": get_RG65_benchmark(),\n",
    "   \"RW\": get_RW_benchmark(),\n",
    "   \"MTurk\": get_MTurk_benchmark()\n",
    "}\n",
    "\n",
    "similarity_results = {}\n",
    "for task, data in iteritems(similarity_tasks):\n",
    "    sim_words, sim_scores = data.X, data.y\n",
    "\n",
    "    human_scores = []\n",
    "    model_scores = []\n",
    "    total_checked = 0\n",
    "\n",
    "    for i, words in enumerate(sim_words):\n",
    "        word1, word2 = words[0], words[1]\n",
    "    \n",
    "        # Check if words exist in embedding dictionary\n",
    "        try:\n",
    "            word1_vec = googlenews_kv[word1]\n",
    "            word2_vec = googlenews_kv[word2]\n",
    "            similarity = 10 * cosine_similarity(word1_vec.reshape(1, -1), word2_vec.reshape(1, -1))\n",
    "            model_similarity = similarity[0][0]     \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "        total_checked += 1\n",
    "        # Store the scores\n",
    "        human_scores.append(sim_scores[i])\n",
    "        model_scores.append(model_similarity)\n",
    "        \n",
    "\n",
    "    model_correlation, p_value = spearmanr(human_scores, model_scores)\n",
    "    print(\"-\" * 50)\n",
    "    print(task)\n",
    "    print('total checked', total_checked, 'out of', len(sim_scores))\n",
    "    print('Corr:',model_correlation)\n",
    "    print('P-Val:',p_value)\n",
    "    \n",
    "    similarity_results[task] = model_correlation\n",
    "    \n",
    "sim = pd.DataFrame([similarity_results])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some classification tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AP  BLESS    BATTIG  ESSLI_2c  ESSLI_2b  ESSLI_1a\n",
      "0  0.639303   0.79  0.382527  0.644444       0.8      0.75\n"
     ]
    }
   ],
   "source": [
    "categorization_tasks = {\n",
    "    \"AP\": get_ap_benchmark(),\n",
    "    \"BLESS\": get_bless_benchmark(),\n",
    "    \"BATTIG\": get_battig_benchmark(),\n",
    "    \"ESSLI_2c\": get_essli_2c_benchmark(),\n",
    "    \"ESSLI_2b\": get_essli_2b_benchmark(),\n",
    "    \"ESSLI_1a\": get_essli_1a_benchmark()\n",
    "}\n",
    "\n",
    "categorization_results = {}\n",
    "w = Embedding.from_gensim_keyedvectors(googlenews_kv, pretrained=True)\n",
    "\n",
    "# Calculate results using helper function\n",
    "for name, data in iteritems(categorization_tasks):\n",
    "    try: \n",
    "        categorization_results[name] = evaluate_categorization(w, data.X, data.y)\n",
    "    except Exception as e:\n",
    "        print(name, e)\n",
    "        continue\n",
    "    \n",
    "# Construct pd table\n",
    "cat = pd.DataFrame([categorization_results])\n",
    "print(cat)\n",
    "del categorization_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text8\n",
    "Now let's evaluate the text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 100\n",
      "Dictionary size 253854\n",
      "Window size 5\n",
      "Total training time 50.620567683596164\n"
     ]
    }
   ],
   "source": [
    "text8_vectors = Word2Vec.load('./out/text8_word2vec_100.model')\n",
    "embedding_info(text8_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19544 questions\n",
      "There are 14 categories\n",
      "Disribution Counter({'capital-world': 4524, 'city-in-state': 2467, 'gram6-nationality-adjective': 1599, 'gram7-past-tense': 1560, 'gram3-comparative': 1332, 'gram8-plural': 1332, 'gram4-superlative': 1122, 'gram5-present-participle': 1056, 'gram1-adjective-to-adverb': 992, 'gram9-plural-verbs': 870, 'currency': 866, 'gram2-opposite': 812, 'capital-common-countries': 506, 'family': 506})\n",
      "Total correct: 649\n",
      "Analogies attempted: 19170\n",
      "Correct_per_category {'capital-world': 8, 'family': 206, 'gram1-adjective-to-adverb': 28, 'gram2-opposite': 89, 'gram3-comparative': 9, 'gram5-present-participle': 77, 'gram7-past-tense': 86, 'gram8-plural': 111, 'gram9-plural-verbs': 26}\n"
     ]
    }
   ],
   "source": [
    "questions, answers, category = get_google_analogy_benchmark() # google analogy benchmark\n",
    "# questions, answers, category = get_msr_benchmark() # msr benchmark\n",
    "\n",
    "subset = range(0, len(questions))\n",
    "correct_total_count, analogies_attempted = 0, 0\n",
    "correct_per_category = {}     # Calculate correct per category\n",
    "\n",
    "for index in tqdm(subset):\n",
    "    word1, word2, word3 = questions[index]\n",
    "    answer = answers[index]\n",
    "\n",
    "    analogy_vec = get_analogy_vec(text8_vectors, word1, word2, word3)\n",
    "    if analogy_vec is False:\n",
    "        continue\n",
    "    \n",
    "    similar_vectors = text8_vectors.wv.most_similar(positive=[analogy_vec])\n",
    "    # Considered correct if answer in top K = 5 of most similar\n",
    "    correct = True if answer in [word for word, _ in similar_vectors[:5]] else False\n",
    "    \n",
    "    analogies_attempted += 1\n",
    "    if correct:\n",
    "        correct_total_count += 1\n",
    "        if category[index] not in correct_per_category:\n",
    "            correct_per_category[category[index]] = 0\n",
    "        else:\n",
    "            correct_per_category[category[index]] += 1\n",
    "\n",
    "print(\"Total correct:\", correct_total_count)\n",
    "print(\"Analogies attempted:\", analogies_attempted)\n",
    "print(\"Correct_per_category\", correct_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "WS353\n",
      "total checked 335 out of 353\n",
      "Corr: 0.6068265046133258\n",
      "P-Val: 4.4385588329947783e-35\n",
      "--------------------------------------------------\n",
      "SimLex999\n",
      "total checked 999 out of 999\n",
      "Corr: 0.25072799617794855\n",
      "P-Val: 8.719873284483485e-16\n",
      "--------------------------------------------------\n",
      "RG65\n",
      "total checked 65 out of 65\n",
      "Corr: 0.49821333195456446\n",
      "P-Val: 2.4082963481106378e-05\n",
      "--------------------------------------------------\n",
      "RW\n",
      "total checked 1530 out of 2034\n",
      "Corr: 0.28203625083558326\n",
      "P-Val: 2.261693553518623e-29\n",
      "--------------------------------------------------\n",
      "MTurk\n",
      "total checked 286 out of 287\n",
      "Corr: 0.6015422128422939\n",
      "P-Val: 1.555016242398753e-29\n",
      "      WS353  SimLex999      RG65        RW     MTurk\n",
      "0  0.606827   0.250728  0.498213  0.282036  0.601542\n"
     ]
    }
   ],
   "source": [
    "similarity_tasks = {\n",
    "   \"WS353\" : get_ws353_benchmark(),\n",
    "   \"SimLex999\": get_simlex999_benchmark(),\n",
    "   \"RG65\": get_RG65_benchmark(),\n",
    "   \"RW\": get_RW_benchmark(),\n",
    "   \"MTurk\": get_MTurk_benchmark()\n",
    "}\n",
    "\n",
    "similarity_results = {}\n",
    "for task, data in iteritems(similarity_tasks):\n",
    "    sim_words, sim_scores = data.X, data.y\n",
    "\n",
    "    human_scores = []\n",
    "    model_scores = []\n",
    "    total_checked = 0\n",
    "\n",
    "    for i, words in enumerate(sim_words):\n",
    "        word1, word2 = words[0], words[1]\n",
    "    \n",
    "        # Check if words exist in embedding dictionary\n",
    "        try:\n",
    "            word1_vec = text8_vectors.wv[word1]\n",
    "            word2_vec = text8_vectors.wv[word2]\n",
    "            similarity = 10 * cosine_similarity(word1_vec.reshape(1, -1), word2_vec.reshape(1, -1))\n",
    "            model_similarity = similarity[0][0]     \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "        total_checked += 1\n",
    "        # Store the scores\n",
    "        human_scores.append(sim_scores[i])\n",
    "        model_scores.append(model_similarity)\n",
    "        \n",
    "\n",
    "    model_correlation, p_value = spearmanr(human_scores, model_scores)\n",
    "    print(\"-\" * 50)\n",
    "    print(task)\n",
    "    print('total checked', total_checked, 'out of', len(sim_scores))\n",
    "    print('Corr:',model_correlation)\n",
    "    print('P-Val:',p_value)\n",
    "    \n",
    "    similarity_results[task] = model_correlation\n",
    "    \n",
    "sim = pd.DataFrame([similarity_results])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AP  BLESS    BATTIG  ESSLI_2c  ESSLI_2b  ESSLI_1a\n",
      "0  0.460199  0.435  0.226152  0.488889       0.8  0.613636\n"
     ]
    }
   ],
   "source": [
    "categorization_tasks = {\n",
    "    \"AP\": get_ap_benchmark(),\n",
    "    \"BLESS\": get_bless_benchmark(),\n",
    "    \"BATTIG\": get_battig_benchmark(),\n",
    "    \"ESSLI_2c\": get_essli_2c_benchmark(),\n",
    "    \"ESSLI_2b\": get_essli_2b_benchmark(),\n",
    "    \"ESSLI_1a\": get_essli_1a_benchmark()\n",
    "}\n",
    "\n",
    "categorization_results = {}\n",
    "w = Embedding.from_gensim_word2vec(text8_vectors)\n",
    "\n",
    "# Calculate results using helper function\n",
    "for name, data in iteritems(categorization_tasks):\n",
    "    try: \n",
    "        categorization_results[name] = evaluate_categorization(w, data.X, data.y)\n",
    "    except Exception as e:\n",
    "        print(name, e)\n",
    "        continue\n",
    "    \n",
    "# Construct pd table\n",
    "cat = pd.DataFrame([categorization_results])\n",
    "print(cat)\n",
    "del categorization_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Twitter 27B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open\n",
      "KeyedVectors<vector_size=100, 1193514 keys>\n"
     ]
    }
   ],
   "source": [
    "# # print(\"Read\")\n",
    "# glove_twitter_kv = load_word_vectors('./data/glove_twitter_27B/glove.twitter.27B.100d_copy.txt')\n",
    "\n",
    "# # add headers to the file\n",
    "# with open('./data/glove_twitter_27B/glove.twitter.27B.100d_copy.txt', 'r') as original: data = original.read()\n",
    "\n",
    "# print(\"Rewrite\")\n",
    "# dictionary_size = len(glove_twitter_kv.items())\n",
    "# with open('./data/glove_twitter_27B/glove.twitter.27B.100d.txt', 'w') as modified: modified.write(f\"{dictionary_size} 100\\n\" + data)\n",
    "\n",
    "print(\"Open\")\n",
    "# Turn <word> <vector> into a Word2Vec object\n",
    "glove_twitter_kv = models.KeyedVectors.load_word2vec_format(\n",
    "    './data/glove_twitter_27B/glove.twitter.27B.100d.txt', binary=False)\n",
    "\n",
    "# Vocab size\n",
    "print(glove_twitter_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8000 questions\n",
      "There are 16 categories\n",
      "Disribution Counter({'jj_jjr': 500, 'jjr_jj': 500, 'jj_jjs': 500, 'jjs_jj': 500, 'jjs_jjr': 500, 'jjr_jjs': 500, 'nn_nns': 500, 'nns_nn': 500, 'nn_nnpos': 500, 'nnpos_nn': 500, 'vb_vbd': 500, 'vbd_vb': 500, 'vb_vbz': 500, 'vbz_vb': 500, 'vbz_vbd': 500, 'vbd_vbz': 500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [07:33<00:00, 17.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 326\n",
      "Analogies attempted: 7682\n",
      "Correct_per_category {'jjr_jj': 0, 'jj_jjs': 10, 'jjs_jj': 7, 'nn_nns': 121, 'nns_nn': 108, 'nn_nnpos': 24, 'nnpos_nn': 19, 'vbz_vb': 16, 'vb_vbd': 3, 'vbd_vb': 2, 'vb_vbz': 4, 'vbd_vbz': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# questions, answers, category = get_google_analogy_benchmark() # google analogy benchmark\n",
    "questions, answers, category = get_msr_benchmark() # msr benchmark\n",
    "\n",
    "subset = range(0, len(questions))\n",
    "correct_total_count = 0         # Calculate total correct\n",
    "analogies_attempted = 0\n",
    "correct_per_category = {}     # Calculate correct per category\n",
    "\n",
    "for index in tqdm(subset):\n",
    "    word1, word2, word3 = questions[index]\n",
    "    answer = answers[index]\n",
    "    \n",
    "    try:\n",
    "        w1_vec = glove_twitter_kv[word1]\n",
    "        w2_vec = glove_twitter_kv[word2]\n",
    "        w3_vec = glove_twitter_kv[word3]\n",
    "        \n",
    "        analogy_vec = w1_vec - w2_vec + w3_vec\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    \n",
    "    # Get similar vectors\n",
    "    similar_vectors = glove_twitter_kv.most_similar(positive=[analogy_vec])\n",
    "    # Considered correct if answer in top K = 5 of most similar\n",
    "    correct = True if answer in [word for word, _ in similar_vectors[:5]] else False\n",
    "        \n",
    "    analogies_attempted += 1\n",
    "    if correct:\n",
    "        correct_total_count += 1\n",
    "        if category[index] not in correct_per_category:\n",
    "            correct_per_category[category[index]] = 0\n",
    "        else:\n",
    "            correct_per_category[category[index]] += 1\n",
    "\n",
    "print(\"Total correct:\", correct_total_count)\n",
    "print(\"Analogies attempted:\", analogies_attempted)\n",
    "print(\"Correct_per_category\", correct_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/ltq/developer/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "WS353\n",
      "total checked 334 out of 353\n",
      "Corr: 0.5212324333414307\n",
      "P-Val: 1.1634914960544721e-24\n",
      "--------------------------------------------------\n",
      "SimLex999\n",
      "total checked 998 out of 999\n",
      "Corr: 0.12031043234601072\n",
      "P-Val: 0.00013906433564553764\n",
      "--------------------------------------------------\n",
      "RG65\n",
      "total checked 65 out of 65\n",
      "Corr: 0.6774486160113895\n",
      "P-Val: 5.746822577586313e-10\n",
      "--------------------------------------------------\n",
      "RW\n",
      "total checked 1063 out of 2034\n",
      "Corr: 0.3264945217909137\n",
      "P-Val: 7.979260997289211e-28\n",
      "--------------------------------------------------\n",
      "MTurk\n",
      "total checked 286 out of 287\n",
      "Corr: 0.5650167476695827\n",
      "P-Val: 1.6012296062189186e-25\n",
      "      WS353  SimLex999      RG65        RW     MTurk\n",
      "0  0.521232    0.12031  0.677449  0.326495  0.565017\n"
     ]
    }
   ],
   "source": [
    "similarity_tasks = {\n",
    "   \"WS353\" : get_ws353_benchmark(),\n",
    "   \"SimLex999\": get_simlex999_benchmark(),\n",
    "   \"RG65\": get_RG65_benchmark(),\n",
    "   \"RW\": get_RW_benchmark(),\n",
    "   \"MTurk\": get_MTurk_benchmark()\n",
    "}\n",
    "\n",
    "similarity_results = {}\n",
    "for task, data in iteritems(similarity_tasks):\n",
    "    sim_words, sim_scores = data.X, data.y\n",
    "\n",
    "    human_scores, model_scores = [], []\n",
    "    total_checked = 0\n",
    "\n",
    "    for i, words in enumerate(sim_words):\n",
    "        word1, word2 = words[0], words[1]\n",
    "    \n",
    "        # Check if words exist in embedding dictionary\n",
    "        try:\n",
    "            word1_vec = glove_twitter_kv[word1]\n",
    "            word2_vec = glove_twitter_kv[word2]\n",
    "            similarity = 10 * cosine_similarity(word1_vec.reshape(1, -1), word2_vec.reshape(1, -1))\n",
    "            model_similarity = similarity[0][0]     \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "        total_checked += 1\n",
    "        # Store the scores\n",
    "        human_scores.append(sim_scores[i])\n",
    "        model_scores.append(model_similarity)\n",
    "        \n",
    "\n",
    "    model_correlation, p_value = spearmanr(human_scores, model_scores)\n",
    "    print(\"-\" * 50)\n",
    "    print(task)\n",
    "    print('total checked', total_checked, 'out of', len(sim_scores))\n",
    "    print('Corr:',model_correlation)\n",
    "    print('P-Val:',p_value)\n",
    "    \n",
    "    similarity_results[task] = model_correlation\n",
    "    \n",
    "sim = pd.DataFrame([similarity_results])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AP  BLESS    BATTIG  ESSLI_2c  ESSLI_2b  ESSLI_1a\n",
      "0  0.462687  0.695  0.318868  0.511111      0.65  0.795455\n"
     ]
    }
   ],
   "source": [
    "categorization_tasks = {\n",
    "    \"AP\": get_ap_benchmark(),\n",
    "    \"BLESS\": get_bless_benchmark(),\n",
    "    \"BATTIG\": get_battig_benchmark(),\n",
    "    \"ESSLI_2c\": get_essli_2c_benchmark(),\n",
    "    \"ESSLI_2b\": get_essli_2b_benchmark(),\n",
    "    \"ESSLI_1a\": get_essli_1a_benchmark()\n",
    "}\n",
    "\n",
    "categorization_results = {}\n",
    "w = Embedding.from_gensim_keyedvectors(glove_twitter_kv, pretrained=True)\n",
    "\n",
    "# Calculate results using helper function\n",
    "for name, data in iteritems(categorization_tasks):\n",
    "    try: \n",
    "        categorization_results[name] = evaluate_categorization(w, data.X, data.y)\n",
    "    except Exception as e:\n",
    "        print(name, e)\n",
    "        continue\n",
    "    \n",
    "# Construct pd table\n",
    "cat = pd.DataFrame([categorization_results])\n",
    "print(cat)\n",
    "del categorization_tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
